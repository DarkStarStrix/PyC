{
  "meta": {
    "timestamp_utc": "2026-02-18T01:09:12.663638+00:00",
    "host": "90c892d22de7",
    "os": "Linux-6.8.0-85-generic-x86_64-with-glibc2.35",
    "python": "3.11.11",
    "device": "cuda",
    "batch": 64,
    "hidden": 2048,
    "iters": 40,
    "warmup": 10,
    "tag": "full_gpu_all",
    "adapters": [
      "torch_eager",
      "torch_compile",
      "pyc",
      "tvm",
      "xla",
      "tensorrt",
      "glow"
    ]
  },
  "gpu": {
    "available": true,
    "gpus": [
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      },
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      }
    ]
  },
  "adapters": {
    "torch_eager": {
      "status": "ok",
      "backend": "torch_eager",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.11,
        "p50": 0.109,
        "p95": 0.1181,
        "min": 0.1072,
        "max": 0.122
      },
      "throughput_tokens_per_sec": 1191112859.45,
      "peak_memory_bytes": 79327744,
      "samples_ms": [
        0.1108,
        0.1098,
        0.1098,
        0.1094,
        0.109,
        0.1089,
        0.1095,
        0.1181,
        0.1139,
        0.1101,
        0.1085,
        0.1089,
        0.1093,
        0.1094,
        0.1102,
        0.1083,
        0.122,
        0.1094,
        0.11,
        0.1086,
        0.1088,
        0.108,
        0.1084,
        0.1088,
        0.1075,
        0.1213,
        0.1078,
        0.1077,
        0.1082,
        0.1094,
        0.1085,
        0.1073,
        0.1073,
        0.1178,
        0.1108,
        0.1092,
        0.1072,
        0.1074,
        0.1086,
        0.1077
      ],
      "adapter": "torch_eager",
      "display_name": "PyTorch Eager"
    },
    "torch_compile": {
      "status": "ok",
      "backend": "torch_compile",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1665,
        "p50": 0.1645,
        "p95": 0.1768,
        "min": 0.1597,
        "max": 0.1914
      },
      "throughput_tokens_per_sec": 787025360.44,
      "peak_memory_bytes": 187326464,
      "samples_ms": [
        0.1693,
        0.1695,
        0.1671,
        0.1694,
        0.1767,
        0.1681,
        0.1674,
        0.1654,
        0.1645,
        0.1664,
        0.1824,
        0.1657,
        0.1608,
        0.1642,
        0.1634,
        0.1628,
        0.1751,
        0.1623,
        0.162,
        0.1645,
        0.1641,
        0.1914,
        0.1651,
        0.1658,
        0.1624,
        0.1645,
        0.1622,
        0.1737,
        0.1641,
        0.1609,
        0.1639,
        0.1604,
        0.164,
        0.1768,
        0.1623,
        0.1597,
        0.1608,
        0.1605,
        0.16,
        0.172
      ],
      "adapter": "torch_compile",
      "display_name": "PyTorch Compile"
    },
    "pyc": {
      "status": "ok",
      "backend": "pyc_cuda_stub",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 141.2764,
        "p50": 141.2184,
        "p95": 146.2196,
        "min": 130.8081,
        "max": 152.2909
      },
      "throughput_tokens_per_sec": 927769.7,
      "peak_memory_bytes": 0,
      "note": "PyC benchmark currently executes deterministic CPU path; CUDA mode is a stub fallback.",
      "adapter": "pyc",
      "display_name": "PyC CUDA"
    },
    "tvm": {
      "status": "unavailable",
      "reason": "TVM installed but CUDA device not available",
      "adapter": "tvm",
      "display_name": "TVM"
    },
    "xla": {
      "status": "unavailable",
      "reason": "torch_xla not installed; install torch_xla or change XLA_BENCH_CMD",
      "adapter": "xla",
      "display_name": "XLA"
    },
    "tensorrt": {
      "status": "unavailable",
      "reason": "torch_tensorrt not installed; install it or change TENSORRT_BENCH_CMD",
      "adapter": "tensorrt",
      "display_name": "TensorRT"
    },
    "glow": {
      "status": "ok",
      "backend": "glow_proxy",
      "device": "cuda",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.111,
        "p50": 0.1095,
        "p95": 0.1226,
        "min": 0.1071,
        "max": 0.1268
      },
      "throughput_tokens_per_sec": 1180886220.46,
      "peak_memory_bytes": 79327744,
      "note": "Glow runtime is not linked in this environment; this is a deterministic proxy workload.",
      "adapter": "glow",
      "display_name": "Glow"
    }
  }
}
