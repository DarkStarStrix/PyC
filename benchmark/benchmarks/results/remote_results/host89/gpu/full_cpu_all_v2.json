{
  "meta": {
    "timestamp_utc": "2026-02-18T01:11:32.964016+00:00",
    "host": "90c892d22de7",
    "os": "Linux-6.8.0-85-generic-x86_64-with-glibc2.35",
    "python": "3.11.11",
    "device": "cpu",
    "batch": 64,
    "hidden": 2048,
    "iters": 40,
    "warmup": 10,
    "tag": "full_cpu_all_v2",
    "adapters": [
      "torch_eager",
      "torch_compile",
      "pyc",
      "tvm",
      "xla",
      "tensorrt",
      "glow"
    ]
  },
  "gpu": {
    "available": true,
    "gpus": [
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      },
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      }
    ]
  },
  "adapters": {
    "torch_eager": {
      "status": "ok",
      "backend": "torch_eager",
      "device": "cpu",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 30.2598,
        "p50": 12.9191,
        "p95": 71.8259,
        "min": 11.9185,
        "max": 72.6317
      },
      "throughput_tokens_per_sec": 4331556.32,
      "peak_memory_bytes": 0,
      "samples_ms": [
        13.0187,
        12.8616,
        70.2576,
        13.3477,
        12.7469,
        71.4011,
        12.7106,
        12.6581,
        12.9911,
        71.96,
        12.9191,
        12.3052,
        70.4454,
        11.937,
        11.9877,
        69.4744,
        14.1555,
        11.9185,
        12.5157,
        71.8259,
        12.489,
        12.1815,
        70.2465,
        12.4055,
        12.0879,
        71.0875,
        13.7756,
        12.3399,
        12.1579,
        69.4699,
        12.1911,
        12.5797,
        71.5981,
        12.6565,
        12.3,
        71.7072,
        13.2764,
        17.5667,
        72.6317,
        12.2055
      ],
      "adapter": "torch_eager",
      "display_name": "PyTorch Eager"
    },
    "torch_compile": {
      "status": "ok",
      "backend": "torch_compile",
      "device": "cpu",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 29.6697,
        "p50": 11.9115,
        "p95": 71.6757,
        "min": 11.407,
        "max": 73.9953
      },
      "throughput_tokens_per_sec": 4417703.78,
      "peak_memory_bytes": 0,
      "samples_ms": [
        11.9115,
        11.9011,
        69.5325,
        12.0091,
        11.6584,
        71.5643,
        13.4334,
        11.407,
        71.6757,
        12.0984,
        11.7501,
        11.8252,
        72.623,
        11.9076,
        12.8715,
        69.4871,
        13.5713,
        11.4796,
        11.5723,
        71.2979,
        11.6719,
        11.6578,
        73.9953,
        11.6757,
        11.7073,
        69.3413,
        12.7426,
        11.6075,
        11.7196,
        70.2125,
        11.7687,
        11.489,
        71.5096,
        13.3633,
        11.7667,
        11.4912,
        68.8112,
        11.5496,
        11.5442,
        71.5866
      ],
      "adapter": "torch_compile",
      "display_name": "PyTorch Compile"
    },
    "pyc": {
      "status": "ok",
      "backend": "pyc_cpu_microbench",
      "device": "cpu",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 141.9295,
        "p50": 141.0531,
        "p95": 146.8969,
        "min": 135.7822,
        "max": 153.9476
      },
      "throughput_tokens_per_sec": 923500.56,
      "peak_memory_bytes": 0,
      "note": "PyC benchmark currently executes deterministic CPU path; CUDA mode is a stub fallback.",
      "adapter": "pyc",
      "display_name": "PyC CUDA"
    },
    "tvm": {
      "status": "error",
      "error": "TVM build/run init failed: Traceback (most recent call last):\n  14: tvm::relay::backend::RelayBuildModule::GetFunction(tvm::runtime::String const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const\n  13: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)\n  12: tvm::relay::backend::RelayBuildModule::OptimizeImpl(tvm::IRModule)\n  11: tvm::transform::Pass::operator()(tvm::IRModule) const\n  10: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  9: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  8: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  7: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  6: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  5: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  4: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::relay::transform::InferType()::{lambda(tvm::IRModule, tvm::transform::PassContext const&)#1}>(tvm::relay::transform::InferType()::{lambda(tvm::IRModule, tvm::transform::PassContext const&)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  3: tvm::relay::TypeInferencer::Infer(tvm::GlobalVar, tvm::relay::Function)\n  2: tvm::relay::TypeSolver::Solve()\n  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<bool (tvm::runtime::Array<tvm::Type, void> const&, int, tvm::Attrs const&, tvm::TypeReporter const&)>::AssignTypedLambda<bool (*)(tvm::runtime::Array<tvm::Type, void> const&, int, tvm::Attrs const&, tvm::TypeReporter const&)>(bool (*)(tvm::runtime::Array<tvm::Type, void> const&, int, tvm::Attrs const&, tvm::TypeReporter const&))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  0: bool tvm::relay::BatchMatmulRel<tvm::relay::BatchMatmulAttrs>(tvm::runtime::Array<tvm::Type, void> const&, int, tvm::Attrs const&, tvm::TypeReporter const&)\n  File \"/workspace/tvm/src/relay/op/nn/nn.h\", line 212\nInternalError: Check failed: (reporter->AssertEQ(xk, yk)) is false: BatchDot: shapes of x and y is inconsistent,  x shape=[64, 1, 2048], y shape=[1, 2048, 64]",
      "adapter": "tvm",
      "display_name": "TVM"
    },
    "xla": {
      "status": "ok",
      "backend": "xla_proxy_torch",
      "device": "cpu",
      "requested_device": "cpu",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 27.5087,
        "p50": 12.2043,
        "p95": 70.1422,
        "min": 9.3482,
        "max": 72.6609
      },
      "throughput_tokens_per_sec": 4764742.16,
      "peak_memory_bytes": 0,
      "note": "torch_xla path unavailable; executed deterministic torch proxy workload.",
      "adapter": "xla",
      "display_name": "XLA"
    },
    "tensorrt": {
      "status": "unavailable",
      "reason": "TensorRT benchmark requires BENCH_DEVICE=cuda",
      "adapter": "tensorrt",
      "display_name": "TensorRT"
    },
    "glow": {
      "status": "ok",
      "backend": "glow_proxy",
      "device": "cpu",
      "requested_device": "cpu",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 34.9326,
        "p50": 15.0033,
        "p95": 73.9626,
        "min": 13.3187,
        "max": 82.8001
      },
      "throughput_tokens_per_sec": 3752137.28,
      "peak_memory_bytes": 0,
      "note": "Glow runtime is not linked in this environment; this is a deterministic proxy workload.",
      "adapter": "glow",
      "display_name": "Glow"
    }
  }
}
