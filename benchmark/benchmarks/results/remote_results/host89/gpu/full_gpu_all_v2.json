{
  "meta": {
    "timestamp_utc": "2026-02-18T01:12:03.818221+00:00",
    "host": "90c892d22de7",
    "os": "Linux-6.8.0-85-generic-x86_64-with-glibc2.35",
    "python": "3.11.11",
    "device": "cuda",
    "batch": 64,
    "hidden": 2048,
    "iters": 40,
    "warmup": 10,
    "tag": "full_gpu_all_v2",
    "adapters": [
      "torch_eager",
      "torch_compile",
      "pyc",
      "tvm",
      "xla",
      "tensorrt",
      "glow"
    ]
  },
  "gpu": {
    "available": true,
    "gpus": [
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      },
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      }
    ]
  },
  "adapters": {
    "torch_eager": {
      "status": "ok",
      "backend": "torch_eager",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1112,
        "p50": 0.1091,
        "p95": 0.1217,
        "min": 0.1066,
        "max": 0.1302
      },
      "throughput_tokens_per_sec": 1179130083.69,
      "peak_memory_bytes": 79327744,
      "samples_ms": [
        0.1217,
        0.114,
        0.1097,
        0.1092,
        0.1091,
        0.1089,
        0.1101,
        0.1093,
        0.1144,
        0.1213,
        0.1084,
        0.1095,
        0.109,
        0.1082,
        0.108,
        0.1066,
        0.1071,
        0.1219,
        0.1124,
        0.1086,
        0.1095,
        0.11,
        0.1075,
        0.1086,
        0.1089,
        0.1083,
        0.1302,
        0.1171,
        0.1098,
        0.1086,
        0.1082,
        0.1079,
        0.1076,
        0.1084,
        0.1194,
        0.1159,
        0.1091,
        0.1084,
        0.1083,
        0.1072
      ],
      "adapter": "torch_eager",
      "display_name": "PyTorch Eager"
    },
    "torch_compile": {
      "status": "ok",
      "backend": "torch_compile",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1602,
        "p50": 0.1599,
        "p95": 0.1743,
        "min": 0.1512,
        "max": 0.1799
      },
      "throughput_tokens_per_sec": 817952787.9,
      "peak_memory_bytes": 78280192,
      "samples_ms": [
        0.1799,
        0.1695,
        0.1666,
        0.1648,
        0.166,
        0.1747,
        0.1743,
        0.1545,
        0.1553,
        0.158,
        0.1545,
        0.1616,
        0.1602,
        0.1525,
        0.1542,
        0.1557,
        0.155,
        0.1532,
        0.156,
        0.1617,
        0.1535,
        0.1536,
        0.1532,
        0.1558,
        0.1658,
        0.1564,
        0.1535,
        0.152,
        0.1624,
        0.1613,
        0.1742,
        0.1673,
        0.1606,
        0.1603,
        0.1599,
        0.1718,
        0.1644,
        0.153,
        0.1512,
        0.1514
      ],
      "adapter": "torch_compile",
      "display_name": "PyTorch Compile"
    },
    "pyc": {
      "status": "ok",
      "backend": "pyc_cuda_stub",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 141.1747,
        "p50": 140.8507,
        "p95": 148.2243,
        "min": 130.5867,
        "max": 154.0757
      },
      "throughput_tokens_per_sec": 928438.01,
      "peak_memory_bytes": 0,
      "note": "PyC benchmark currently executes deterministic CPU path; CUDA mode is a stub fallback.",
      "adapter": "pyc",
      "display_name": "PyC CUDA"
    },
    "tvm": {
      "status": "error",
      "error": "TVM build/run init failed: Traceback (most recent call last):\n  14: tvm::relay::backend::RelayBuildModule::GetFunction(tvm::runtime::String const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const\n  13: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)\n  12: tvm::relay::backend::RelayBuildModule::OptimizeImpl(tvm::IRModule)\n  11: tvm::transform::Pass::operator()(tvm::IRModule) const\n  10: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  9: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  8: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  7: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  6: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  5: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  4: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::relay::transform::InferType()::{lambda(tvm::IRModule, tvm::transform::PassContext const&)#1}>(tvm::relay::transform::InferType()::{lambda(tvm::IRModule, tvm::transform::PassContext const&)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  3: tvm::relay::TypeInferencer::Infer(tvm::GlobalVar, tvm::relay::Function)\n  2: tvm::relay::TypeSolver::Solve()\n  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<bool (tvm::runtime::Array<tvm::Type, void> const&, int, tvm::Attrs const&, tvm::TypeReporter const&)>::AssignTypedLambda<bool (*)(tvm::runtime::Array<tvm::Type, void> const&, int, tvm::Attrs const&, tvm::TypeReporter const&)>(bool (*)(tvm::runtime::Array<tvm::Type, void> const&, int, tvm::Attrs const&, tvm::TypeReporter const&))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  0: bool tvm::relay::BatchMatmulRel<tvm::relay::BatchMatmulAttrs>(tvm::runtime::Array<tvm::Type, void> const&, int, tvm::Attrs const&, tvm::TypeReporter const&)\n  File \"/workspace/tvm/src/relay/op/nn/nn.h\", line 212\nInternalError: Check failed: (reporter->AssertEQ(xk, yk)) is false: BatchDot: shapes of x and y is inconsistent,  x shape=[64, 1, 2048], y shape=[1, 2048, 64]",
      "adapter": "tvm",
      "display_name": "TVM"
    },
    "xla": {
      "status": "ok",
      "backend": "xla_proxy_torch",
      "device": "cuda",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1149,
        "p50": 0.1111,
        "p95": 0.1287,
        "min": 0.1081,
        "max": 0.1366
      },
      "throughput_tokens_per_sec": 1140711821.71,
      "peak_memory_bytes": 79327744,
      "note": "torch_xla path unavailable; executed deterministic torch proxy workload.",
      "adapter": "xla",
      "display_name": "XLA"
    },
    "tensorrt": {
      "status": "ok",
      "backend": "tensorrt_proxy_torch_compile",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.2057,
        "p50": 0.2049,
        "p95": 0.2147,
        "min": 0.1945,
        "max": 0.2412
      },
      "throughput_tokens_per_sec": 637328770.22,
      "peak_memory_bytes": 153505792,
      "note": "torch_tensorrt unavailable; executed torch.compile proxy workload.",
      "adapter": "tensorrt",
      "display_name": "TensorRT"
    },
    "glow": {
      "status": "ok",
      "backend": "glow_proxy",
      "device": "cuda",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1107,
        "p50": 0.1093,
        "p95": 0.121,
        "min": 0.1062,
        "max": 0.1258
      },
      "throughput_tokens_per_sec": 1183536227.46,
      "peak_memory_bytes": 79327744,
      "note": "Glow runtime is not linked in this environment; this is a deterministic proxy workload.",
      "adapter": "glow",
      "display_name": "Glow"
    }
  }
}
