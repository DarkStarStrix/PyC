{
  "meta": {
    "timestamp_utc": "2026-02-18T02:34:26.541928+00:00",
    "host": "90c892d22de7",
    "os": "Linux-6.8.0-85-generic-x86_64-with-glibc2.35",
    "python": "3.11.11",
    "device": "cuda",
    "batch": 64,
    "hidden": 2048,
    "iters": 40,
    "warmup": 10,
    "tag": "gpu",
    "adapters": [
      "torch_eager",
      "torch_compile",
      "pyc",
      "tvm",
      "xla",
      "tensorrt",
      "glow"
    ],
    "run_id": "20260218T023355Z_phase4_final",
    "git_head": "unknown",
    "git_dirty": 0
  },
  "gpu": {
    "available": true,
    "gpus": [
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      },
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      }
    ]
  },
  "adapters": {
    "torch_eager": {
      "status": "ok",
      "backend": "torch_eager",
      "mode": "native",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1154,
        "p50": 0.1135,
        "p95": 0.1285,
        "min": 0.1115,
        "max": 0.1487
      },
      "throughput_tokens_per_sec": 1135905416.11,
      "peak_memory_bytes": 78541312,
      "samples_ms": [
        0.1176,
        0.1148,
        0.1142,
        0.1142,
        0.1139,
        0.1137,
        0.114,
        0.1256,
        0.1175,
        0.1143,
        0.1126,
        0.1135,
        0.1129,
        0.1133,
        0.1122,
        0.1137,
        0.1288,
        0.1131,
        0.1132,
        0.1127,
        0.1141,
        0.1134,
        0.1123,
        0.1135,
        0.1285,
        0.1133,
        0.1123,
        0.1115,
        0.1131,
        0.1118,
        0.1119,
        0.1115,
        0.1487,
        0.1169,
        0.1139,
        0.1134,
        0.113,
        0.1136,
        0.1117,
        0.1118
      ],
      "adapter": "torch_eager",
      "display_name": "PyTorch Eager"
    },
    "torch_compile": {
      "status": "ok",
      "backend": "torch_compile",
      "mode": "native",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1551,
        "p50": 0.1544,
        "p95": 0.1665,
        "min": 0.1495,
        "max": 0.169
      },
      "throughput_tokens_per_sec": 844896466.85,
      "peak_memory_bytes": 76191232,
      "samples_ms": [
        0.1562,
        0.1593,
        0.1547,
        0.1665,
        0.1554,
        0.1606,
        0.1534,
        0.1561,
        0.1528,
        0.1639,
        0.1569,
        0.152,
        0.1546,
        0.1534,
        0.1507,
        0.1619,
        0.1564,
        0.1523,
        0.1516,
        0.1503,
        0.1511,
        0.1509,
        0.1516,
        0.1511,
        0.1499,
        0.169,
        0.1551,
        0.1496,
        0.1667,
        0.1536,
        0.156,
        0.158,
        0.1495,
        0.1544,
        0.1613,
        0.1501,
        0.1505,
        0.1508,
        0.1549,
        0.1525
      ],
      "adapter": "torch_compile",
      "display_name": "PyTorch Compile"
    },
    "pyc": {
      "status": "ok",
      "backend": "pyc_compiler_next",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 25.5228,
        "p50": 25.383,
        "p95": 27.327,
        "min": 23.869,
        "max": 28.292
      },
      "throughput_tokens_per_sec": 5135486.7,
      "peak_memory_bytes": 17825792,
      "profile": {
        "dispatch_ms_mean": 25.5136,
        "graph_exec_ms_mean": 25.5136,
        "controller_ms_mean": 0.0004,
        "kernel_select_ms_mean": 0.001
      },
      "reliability": {
        "compile_cache_hit": 0,
        "compile_budget_exceeded": 0,
        "guard_miss_count": 0,
        "fallback_count": 50,
        "graph_break_count": 0,
        "compilability_score": 1.0,
        "autotune_loaded": 0,
        "autotune_saved": 0
      },
      "mode": "proxy",
      "note": "PyC benchmark uses compiler-next API path; CUDA currently executes deterministic fallback until native kernels land.",
      "adapter": "pyc",
      "display_name": "PyC CUDA"
    },
    "tvm": {
      "status": "unavailable",
      "reason": "TVM not installed; install TVM or change TVM_BENCH_CMD",
      "adapter": "tvm",
      "display_name": "TVM",
      "mode": "unknown"
    },
    "xla": {
      "status": "ok",
      "backend": "xla_proxy_torch",
      "mode": "proxy",
      "device": "cuda",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1157,
        "p50": 0.1138,
        "p95": 0.1263,
        "min": 0.1114,
        "max": 0.1372
      },
      "throughput_tokens_per_sec": 1133048992.77,
      "peak_memory_bytes": 78541312,
      "note": "torch_xla path unavailable; executed deterministic torch proxy workload.",
      "adapter": "xla",
      "display_name": "XLA"
    },
    "tensorrt": {
      "status": "ok",
      "backend": "tensorrt_proxy_torch_compile",
      "mode": "proxy",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1598,
        "p50": 0.156,
        "p95": 0.1743,
        "min": 0.1518,
        "max": 0.1779
      },
      "throughput_tokens_per_sec": 820055823.27,
      "peak_memory_bytes": 153505792,
      "note": "torch_tensorrt unavailable; executed torch.compile proxy workload.",
      "adapter": "tensorrt",
      "display_name": "TensorRT"
    },
    "glow": {
      "status": "ok",
      "backend": "glow_proxy",
      "mode": "proxy",
      "device": "cuda",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1314,
        "p50": 0.119,
        "p95": 0.1332,
        "min": 0.1176,
        "max": 0.3455
      },
      "throughput_tokens_per_sec": 997355892.15,
      "peak_memory_bytes": 78541312,
      "note": "Glow runtime is not linked in this environment; this is a deterministic proxy workload.",
      "adapter": "glow",
      "display_name": "Glow"
    }
  }
}
