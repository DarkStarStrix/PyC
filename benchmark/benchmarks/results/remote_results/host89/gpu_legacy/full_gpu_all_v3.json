{
  "meta": {
    "timestamp_utc": "2026-02-18T01:14:14.081940+00:00",
    "host": "90c892d22de7",
    "os": "Linux-6.8.0-85-generic-x86_64-with-glibc2.35",
    "python": "3.11.11",
    "device": "cuda",
    "batch": 64,
    "hidden": 2048,
    "iters": 30,
    "warmup": 10,
    "tag": "full_gpu_all_v3",
    "adapters": [
      "torch_eager",
      "torch_compile",
      "pyc",
      "tvm",
      "xla",
      "tensorrt",
      "glow"
    ]
  },
  "gpu": {
    "available": true,
    "gpus": [
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      },
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      }
    ]
  },
  "adapters": {
    "torch_eager": {
      "status": "ok",
      "backend": "torch_eager",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 30,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1142,
        "p50": 0.1074,
        "p95": 0.1207,
        "min": 0.1051,
        "max": 0.2782
      },
      "throughput_tokens_per_sec": 1148051483.68,
      "peak_memory_bytes": 79327744,
      "samples_ms": [
        0.1106,
        0.1099,
        0.1093,
        0.1082,
        0.1089,
        0.109,
        0.108,
        0.1207,
        0.1089,
        0.1084,
        0.1075,
        0.2782,
        0.1066,
        0.1058,
        0.1152,
        0.1072,
        0.1074,
        0.1057,
        0.106,
        0.1062,
        0.1051,
        0.1064,
        0.1065,
        0.1192,
        0.106,
        0.1065,
        0.1062,
        0.1069,
        0.1071,
        0.1074
      ],
      "adapter": "torch_eager",
      "display_name": "PyTorch Eager"
    },
    "torch_compile": {
      "status": "ok",
      "backend": "torch_compile",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 30,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1588,
        "p50": 0.1565,
        "p95": 0.17,
        "min": 0.1527,
        "max": 0.1716
      },
      "throughput_tokens_per_sec": 825641584.09,
      "peak_memory_bytes": 78280192,
      "samples_ms": [
        0.1635,
        0.161,
        0.1588,
        0.17,
        0.1587,
        0.1572,
        0.1552,
        0.1555,
        0.1595,
        0.1695,
        0.1576,
        0.156,
        0.1575,
        0.1567,
        0.1565,
        0.1698,
        0.1561,
        0.1543,
        0.155,
        0.156,
        0.1558,
        0.1716,
        0.1565,
        0.1584,
        0.1548,
        0.1538,
        0.1542,
        0.167,
        0.1535,
        0.1527
      ],
      "adapter": "torch_compile",
      "display_name": "PyTorch Compile"
    },
    "pyc": {
      "status": "ok",
      "backend": "pyc_cuda_stub",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 30,
      "warmup": 10,
      "latency_ms": {
        "mean": 153.8173,
        "p50": 152.0273,
        "p95": 170.1069,
        "min": 138.8631,
        "max": 176.5654
      },
      "throughput_tokens_per_sec": 852127.69,
      "peak_memory_bytes": 0,
      "note": "PyC benchmark currently executes deterministic CPU path; CUDA mode is a stub fallback.",
      "adapter": "pyc",
      "display_name": "PyC CUDA"
    },
    "tvm": {
      "status": "ok",
      "backend": "tvm",
      "device": "cpu",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 30,
      "warmup": 10,
      "latency_ms": {
        "mean": 28.8425,
        "p50": 11.5654,
        "p95": 70.617,
        "min": 10.1491,
        "max": 70.8559
      },
      "throughput_tokens_per_sec": 4544398.42,
      "peak_memory_bytes": 0,
      "note": "TVM CUDA target unavailable; benchmark executed on TVM CPU fallback.",
      "adapter": "tvm",
      "display_name": "TVM"
    },
    "xla": {
      "status": "ok",
      "backend": "xla_proxy_torch",
      "device": "cuda",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 30,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1188,
        "p50": 0.114,
        "p95": 0.1386,
        "min": 0.1125,
        "max": 0.1401
      },
      "throughput_tokens_per_sec": 1103033291.36,
      "peak_memory_bytes": 79327744,
      "note": "torch_xla path unavailable; executed deterministic torch proxy workload.",
      "adapter": "xla",
      "display_name": "XLA"
    },
    "tensorrt": {
      "status": "ok",
      "backend": "tensorrt_proxy_torch_compile",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 30,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1693,
        "p50": 0.166,
        "p95": 0.1907,
        "min": 0.161,
        "max": 0.2206
      },
      "throughput_tokens_per_sec": 774318447.61,
      "peak_memory_bytes": 78541312,
      "note": "torch_tensorrt unavailable; executed torch.compile proxy workload.",
      "adapter": "tensorrt",
      "display_name": "TensorRT"
    },
    "glow": {
      "status": "ok",
      "backend": "glow_proxy",
      "device": "cuda",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 30,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1157,
        "p50": 0.1142,
        "p95": 0.1286,
        "min": 0.1128,
        "max": 0.1292
      },
      "throughput_tokens_per_sec": 1132491308.25,
      "peak_memory_bytes": 79327744,
      "note": "Glow runtime is not linked in this environment; this is a deterministic proxy workload.",
      "adapter": "glow",
      "display_name": "Glow"
    }
  }
}
