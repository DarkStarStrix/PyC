# GPU Benchmark Suite

- Timestamp (UTC): 2026-02-18T02:33:57.325322+00:00
- Host: 90c892d22de7
- OS: Linux-6.8.0-85-generic-x86_64-with-glibc2.35
- Python: 3.11.11

## GPU

- GPU 1: NVIDIA GeForce RTX 4090 | driver 570.195.03 | mem 24564 MiB | cc 8.9
- GPU 2: NVIDIA GeForce RTX 4090 | driver 570.195.03 | mem 24564 MiB | cc 8.9

## Adapter Results

- `PyTorch Eager` [native]: p50 15.4305 ms, p95 75.9299 ms, mean 36.9971 ms, throughput 3542766.92 tokens/s
- `PyTorch Compile` [native]: p50 10.0514 ms, p95 72.1404 ms, mean 26.7237 ms, throughput 4904708.07 tokens/s
- `PyC CUDA` [native]: p50 24.022 ms, p95 24.18 ms, mean 24.0459 ms, throughput 5450908.47 tokens/s
- `TVM`: unavailable (TVM not installed; install TVM or change TVM_BENCH_CMD)
- `XLA` [proxy]: p50 12.8271 ms, p95 72.3854 ms, mean 30.2406 ms, throughput 4334310.06 tokens/s
- `TensorRT` [proxy]: p50 9.3322 ms, p95 68.6434 ms, mean 24.4121 ms, throughput 5369130.72 tokens/s
- `Glow` [proxy]: p50 15.7008 ms, p95 75.1887 ms, mean 34.8523 ms, throughput 3760782.37 tokens/s

## Notes

- Adapters are normalized to a common JSON schema.
- For TVM/XLA/TensorRT/PyC custom paths, configure `*_BENCH_CMD` env vars in each adapter.
