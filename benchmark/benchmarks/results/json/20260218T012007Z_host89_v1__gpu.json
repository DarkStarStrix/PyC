{
  "meta": {
    "timestamp_utc": "2026-02-18T01:20:47.810820+00:00",
    "host": "90c892d22de7",
    "os": "Linux-6.8.0-85-generic-x86_64-with-glibc2.35",
    "python": "3.11.11",
    "device": "cuda",
    "batch": 64,
    "hidden": 2048,
    "iters": 40,
    "warmup": 10,
    "tag": "gpu",
    "adapters": [
      "torch_eager",
      "torch_compile",
      "pyc",
      "tvm",
      "xla",
      "tensorrt",
      "glow"
    ],
    "run_id": "20260218T012007Z_host89_v1",
    "git_head": "unknown",
    "git_dirty": 0
  },
  "gpu": {
    "available": true,
    "gpus": [
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      },
      {
        "name": "NVIDIA GeForce RTX 4090",
        "driver_version": "570.195.03",
        "memory_total": "24564 MiB",
        "compute_capability": "8.9"
      }
    ]
  },
  "adapters": {
    "torch_eager": {
      "status": "ok",
      "backend": "torch_eager",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1184,
        "p50": 0.1171,
        "p95": 0.1275,
        "min": 0.1149,
        "max": 0.1333
      },
      "throughput_tokens_per_sec": 1106563375.79,
      "peak_memory_bytes": 79327744,
      "samples_ms": [
        0.1199,
        0.1193,
        0.1185,
        0.1192,
        0.1333,
        0.122,
        0.1183,
        0.1169,
        0.1179,
        0.1176,
        0.1159,
        0.116,
        0.1275,
        0.1179,
        0.1156,
        0.1158,
        0.1152,
        0.1155,
        0.1151,
        0.115,
        0.1268,
        0.1186,
        0.1158,
        0.1156,
        0.1156,
        0.1167,
        0.1159,
        0.1173,
        0.1275,
        0.121,
        0.1185,
        0.1171,
        0.1163,
        0.116,
        0.1164,
        0.115,
        0.1251,
        0.1192,
        0.1162,
        0.1149
      ],
      "adapter": "torch_eager",
      "display_name": "PyTorch Eager"
    },
    "torch_compile": {
      "status": "ok",
      "backend": "torch_compile",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1604,
        "p50": 0.1598,
        "p95": 0.171,
        "min": 0.1516,
        "max": 0.1903
      },
      "throughput_tokens_per_sec": 816905665.59,
      "peak_memory_bytes": 78280192,
      "samples_ms": [
        0.1631,
        0.1679,
        0.1579,
        0.1591,
        0.1675,
        0.1598,
        0.1606,
        0.1599,
        0.1626,
        0.1608,
        0.171,
        0.1625,
        0.1615,
        0.158,
        0.1602,
        0.1623,
        0.1777,
        0.1617,
        0.1598,
        0.155,
        0.1533,
        0.1525,
        0.1654,
        0.1562,
        0.1554,
        0.1542,
        0.158,
        0.1903,
        0.1693,
        0.1552,
        0.1551,
        0.156,
        0.154,
        0.1533,
        0.1671,
        0.1611,
        0.1516,
        0.1544,
        0.153,
        0.1537
      ],
      "adapter": "torch_compile",
      "display_name": "PyTorch Compile"
    },
    "pyc": {
      "status": "ok",
      "backend": "pyc_cuda_stub",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 146.3051,
        "p50": 147.8003,
        "p95": 152.0208,
        "min": 131.6009,
        "max": 164.7627
      },
      "throughput_tokens_per_sec": 895881.02,
      "peak_memory_bytes": 0,
      "note": "PyC benchmark currently executes deterministic CPU path; CUDA mode is a stub fallback.",
      "adapter": "pyc",
      "display_name": "PyC CUDA"
    },
    "tvm": {
      "status": "ok",
      "backend": "tvm",
      "device": "cpu",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 24.7123,
        "p50": 10.0789,
        "p95": 69.3395,
        "min": 9.7067,
        "max": 69.5698
      },
      "throughput_tokens_per_sec": 5303906.98,
      "peak_memory_bytes": 0,
      "note": "TVM CUDA target unavailable; benchmark executed on TVM CPU fallback.",
      "adapter": "tvm",
      "display_name": "TVM"
    },
    "xla": {
      "status": "ok",
      "backend": "xla_proxy_torch",
      "device": "cuda",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1263,
        "p50": 0.1253,
        "p95": 0.1507,
        "min": 0.1078,
        "max": 0.1666
      },
      "throughput_tokens_per_sec": 1037486165.78,
      "peak_memory_bytes": 79327744,
      "note": "torch_xla path unavailable; executed deterministic torch proxy workload.",
      "adapter": "xla",
      "display_name": "XLA"
    },
    "tensorrt": {
      "status": "ok",
      "backend": "tensorrt_proxy_torch_compile",
      "device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.2134,
        "p50": 0.171,
        "p95": 0.4429,
        "min": 0.1626,
        "max": 0.826
      },
      "throughput_tokens_per_sec": 614332341.9,
      "peak_memory_bytes": 78541312,
      "note": "torch_tensorrt unavailable; executed torch.compile proxy workload.",
      "adapter": "tensorrt",
      "display_name": "TensorRT"
    },
    "glow": {
      "status": "ok",
      "backend": "glow_proxy",
      "device": "cuda",
      "requested_device": "cuda",
      "batch": 64,
      "hidden": 2048,
      "iters": 40,
      "warmup": 10,
      "latency_ms": {
        "mean": 0.1186,
        "p50": 0.1157,
        "p95": 0.134,
        "min": 0.1133,
        "max": 0.1575
      },
      "throughput_tokens_per_sec": 1105361407.6,
      "peak_memory_bytes": 79327744,
      "note": "Glow runtime is not linked in this environment; this is a deterministic proxy workload.",
      "adapter": "glow",
      "display_name": "Glow"
    }
  }
}
