# GPU Benchmark Suite

- Timestamp (UTC): 2026-02-18T02:02:44.668436+00:00
- Host: 90c892d22de7
- OS: Linux-6.8.0-85-generic-x86_64-with-glibc2.35
- Python: 3.11.11

## GPU

- GPU 1: NVIDIA GeForce RTX 4090 | driver 570.195.03 | mem 24564 MiB | cc 8.9
- GPU 2: NVIDIA GeForce RTX 4090 | driver 570.195.03 | mem 24564 MiB | cc 8.9

## Adapter Results

- `PyTorch Eager` [native]: p50 0.1136 ms, p95 0.126 ms, mean 0.1156 ms, throughput 1134000754.63 tokens/s
- `PyTorch Compile` [native]: p50 0.1528 ms, p95 0.1746 ms, mean 0.193 ms, throughput 679055222.86 tokens/s
- `PyC CUDA` [proxy]: p50 1074.713 ms, p95 1152.099 ms, mean 1092.7849 ms, throughput 119943.09 tokens/s
- `TVM` [proxy]: p50 14.6102 ms, p95 73.542 ms, mean 32.9062 ms, throughput 3983198.89 tokens/s
- `XLA` [proxy]: p50 0.1102 ms, p95 0.1217 ms, mean 0.1117 ms, throughput 1173193201.92 tokens/s
- `TensorRT` [proxy]: p50 0.1574 ms, p95 0.1757 ms, mean 0.1603 ms, throughput 817597590.94 tokens/s
- `Glow` [proxy]: p50 0.1049 ms, p95 0.1159 ms, mean 0.1063 ms, throughput 1233192595.41 tokens/s

## Notes

- Adapters are normalized to a common JSON schema.
- For TVM/XLA/TensorRT/PyC custom paths, configure `*_BENCH_CMD` env vars in each adapter.
