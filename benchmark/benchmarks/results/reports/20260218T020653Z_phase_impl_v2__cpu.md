# GPU Benchmark Suite

- Timestamp (UTC): 2026-02-18T02:06:53.599453+00:00
- Host: 90c892d22de7
- OS: Linux-6.8.0-85-generic-x86_64-with-glibc2.35
- Python: 3.11.11

## GPU

- GPU 1: NVIDIA GeForce RTX 4090 | driver 570.195.03 | mem 24564 MiB | cc 8.9
- GPU 2: NVIDIA GeForce RTX 4090 | driver 570.195.03 | mem 24564 MiB | cc 8.9

## Adapter Results

- `PyTorch Eager` [native]: p50 14.5852 ms, p95 73.8403 ms, mean 34.6332 ms, throughput 3784576.88 tokens/s
- `PyTorch Compile` [native]: p50 13.9565 ms, p95 73.861 ms, mean 32.881 ms, throughput 3986249.41 tokens/s
- `PyC CUDA` [native]: p50 888.972 ms, p95 1238.539 ms, mean 941.6049 ms, throughput 139200.64 tokens/s
- `TVM` [native]: p50 81.9292 ms, p95 95.3797 ms, mean 62.6453 ms, throughput 2092287.61 tokens/s
- `XLA` [proxy]: p50 18.3213 ms, p95 76.4649 ms, mean 39.8538 ms, throughput 3288821.21 tokens/s
- `TensorRT` [proxy]: p50 9.8483 ms, p95 68.7295 ms, mean 23.2356 ms, throughput 5641006.61 tokens/s
- `Glow` [proxy]: p50 19.195 ms, p95 76.1299 ms, mean 39.2014 ms, throughput 3343558.07 tokens/s

## Notes

- Adapters are normalized to a common JSON schema.
- For TVM/XLA/TensorRT/PyC custom paths, configure `*_BENCH_CMD` env vars in each adapter.
