# GPU Benchmark Suite

- Timestamp (UTC): 2026-02-18T02:07:32.066778+00:00
- Host: 90c892d22de7
- OS: Linux-6.8.0-85-generic-x86_64-with-glibc2.35
- Python: 3.11.11

## GPU

- GPU 1: NVIDIA GeForce RTX 4090 | driver 570.195.03 | mem 24564 MiB | cc 8.9
- GPU 2: NVIDIA GeForce RTX 4090 | driver 570.195.03 | mem 24564 MiB | cc 8.9

## Adapter Results

- `PyTorch Eager` [native]: p50 0.1088 ms, p95 0.1172 ms, mean 0.1102 ms, throughput 1189558411.5 tokens/s
- `PyTorch Compile` [native]: p50 0.1584 ms, p95 0.17 ms, mean 0.1603 ms, throughput 817598540.89 tokens/s
- `PyC CUDA` [proxy]: p50 959.754 ms, p95 1315.639 ms, mean 987.6086 ms, throughput 132716.55 tokens/s
- `TVM` [proxy]: p50 10.1201 ms, p95 71.0353 ms, mean 25.1754 ms, throughput 5206350.05 tokens/s
- `XLA` [proxy]: p50 0.1092 ms, p95 0.1191 ms, mean 0.1108 ms, throughput 1183181509.92 tokens/s
- `TensorRT` [proxy]: p50 0.1649 ms, p95 0.1754 ms, mean 0.1662 ms, throughput 788514887.76 tokens/s
- `Glow` [proxy]: p50 0.1166 ms, p95 0.1297 ms, mean 0.1181 ms, throughput 1109554832.69 tokens/s

## Notes

- Adapters are normalized to a common JSON schema.
- For TVM/XLA/TensorRT/PyC custom paths, configure `*_BENCH_CMD` env vars in each adapter.
