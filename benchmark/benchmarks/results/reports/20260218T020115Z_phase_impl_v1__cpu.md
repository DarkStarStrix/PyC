# GPU Benchmark Suite

- Timestamp (UTC): 2026-02-18T02:01:15.498702+00:00
- Host: 90c892d22de7
- OS: Linux-6.8.0-85-generic-x86_64-with-glibc2.35
- Python: 3.11.11

## GPU

- GPU 1: NVIDIA GeForce RTX 4090 | driver 570.195.03 | mem 24564 MiB | cc 8.9
- GPU 2: NVIDIA GeForce RTX 4090 | driver 570.195.03 | mem 24564 MiB | cc 8.9

## Adapter Results

- `PyTorch Eager` [native]: p50 16.5592 ms, p95 76.1947 ms, mean 36.7279 ms, throughput 3568730.16 tokens/s
- `PyTorch Compile` [native]: p50 10.7323 ms, p95 70.709 ms, mean 25.3152 ms, throughput 5177599.72 tokens/s
- `PyC CUDA` [native]: p50 1068.399 ms, p95 1133.616 ms, mean 1080.387 ms, throughput 121319.49 tokens/s
- `TVM` [native]: p50 82.076 ms, p95 87.5437 ms, mean 63.086 ms, throughput 2077673.06 tokens/s
- `XLA` [proxy]: p50 19.3143 ms, p95 76.3604 ms, mean 39.5868 ms, throughput 3311002.96 tokens/s
- `TensorRT` [proxy]: p50 10.1843 ms, p95 70.0699 ms, mean 25.2202 ms, throughput 5197095.92 tokens/s
- `Glow` [proxy]: p50 16.2324 ms, p95 77.0203 ms, mean 35.5114 ms, throughput 3690981.62 tokens/s

## Notes

- Adapters are normalized to a common JSON schema.
- For TVM/XLA/TensorRT/PyC custom paths, configure `*_BENCH_CMD` env vars in each adapter.
